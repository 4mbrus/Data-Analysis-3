{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991f0550",
   "metadata": {},
   "source": [
    "### PART I: Probability prediction\n",
    "- Predict probabilities.\n",
    "- Look at cross-validated performance and pick your favorite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from statsmodels.tools.eval_measures import mse,rmse\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "import sklearn.metrics as metrics\n",
    "import patsy\n",
    "from stargazer.stargazer import Stargazer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54199ddf",
   "metadata": {},
   "source": [
    "# PART I: Probability prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4966b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the clean dataset\n",
    "firms_df = pd.read_csv(\"bisnode_firms_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd3625ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawvars = [\"curr_assets\", \"curr_liab\", \"extra_exp\", \"extra_inc\", \"extra_profit_loss\", \"fixed_assets\",\n",
    "              \"inc_bef_tax\", \"intang_assets\", \"inventories\", \"liq_assets\", \"material_exp\", \"personnel_exp\",\n",
    "              \"profit_loss_year\", \"sales\", \"share_eq\", \"subscribed_cap\"]\n",
    "\n",
    "qualityvars = [\"balsheet_flag\", \"balsheet_length\", \"balsheet_notfullyear\"]\n",
    "\n",
    "engvar = [\"total_assets_bs\", \"fixed_assets_bs\", \"liq_assets_bs\", \"curr_assets_bs\",\n",
    "            \"share_eq_bs\", \"subscribed_cap_bs\", \"intang_assets_bs\", \"extra_exp_pl\",\n",
    "            \"extra_inc_pl\", \"extra_profit_loss_pl\", \"inc_bef_tax_pl\", \"inventories_pl\",\n",
    "            \"material_exp_pl\", \"profit_loss_year_pl\", \"personnel_exp_pl\"]\n",
    "\n",
    "engvar2 = [\"extra_profit_loss_pl_quad\", \"inc_bef_tax_pl_quad\",\n",
    "             \"profit_loss_year_pl_quad\", \"share_eq_bs_quad\"]\n",
    "\n",
    "engvar3 = []\n",
    "for col in firms_df.columns:\n",
    "    if col.endswith('flag_low') or col.endswith('flag_high') or col.endswith('flag_error') or col.endswith('flag_zero'):\n",
    "        engvar3.append(col)\n",
    "\n",
    "d1 =  [\"d1_sales_mil_log_mod\", \"d1_sales_mil_log_mod_sq\",\n",
    "         \"flag_low_d1_sales_mil_log\", \"flag_high_d1_sales_mil_log\"]\n",
    "\n",
    "hr = [\"female\", \"ceo_age\", \"flag_high_ceo_age\", \"flag_low_ceo_age\",\n",
    "        \"flag_miss_ceo_age\", \"ceo_count\", \"labor_avg_mod\",\n",
    "        \"flag_miss_labor_avg\", \"foreign_management\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663b4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = rawvars + qualityvars + engvar + engvar2 + engvar3 + d1 + hr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6c93f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "curr_assets            0\n",
       "curr_liab              0\n",
       "extra_exp              0\n",
       "extra_inc              0\n",
       "extra_profit_loss      0\n",
       "                      ..\n",
       "flag_miss_ceo_age      0\n",
       "ceo_count              0\n",
       "labor_avg_mod          0\n",
       "flag_miss_labor_avg    0\n",
       "foreign_management     0\n",
       "Length: 78, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firms_df[all_vars].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5f52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "firms_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c7095",
   "metadata": {},
   "source": [
    "### Dealing with categorical variables\n",
    "To avoide multicolinearity, we drop the first values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d6b1a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>comp_id</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>amort</th>\n",
       "      <th>curr_assets</th>\n",
       "      <th>curr_liab</th>\n",
       "      <th>extra_exp</th>\n",
       "      <th>extra_inc</th>\n",
       "      <th>extra_profit_loss</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_high_ceo_age</th>\n",
       "      <th>flag_miss_ceo_age</th>\n",
       "      <th>ceo_young</th>\n",
       "      <th>labor_avg_mod</th>\n",
       "      <th>flag_miss_labor_avg</th>\n",
       "      <th>sales_mil_log_sq</th>\n",
       "      <th>flag_low_d1_sales_mil_log</th>\n",
       "      <th>flag_high_d1_sales_mil_log</th>\n",
       "      <th>d1_sales_mil_log_mod</th>\n",
       "      <th>d1_sales_mil_log_mod_sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>1002029.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>14255.555664</td>\n",
       "      <td>217103.703125</td>\n",
       "      <td>161174.078125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0</td>\n",
       "      <td>1.054824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.155013</td>\n",
       "      <td>1.334055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>1011889.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>66125.929688</td>\n",
       "      <td>235114.812500</td>\n",
       "      <td>16555.554688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019109</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>1014183.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>6970.370605</td>\n",
       "      <td>209562.968750</td>\n",
       "      <td>5703.703613</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.819444</td>\n",
       "      <td>0</td>\n",
       "      <td>4.632597</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.110044</td>\n",
       "      <td>0.012110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>1022796.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>503.703705</td>\n",
       "      <td>3859.259277</td>\n",
       "      <td>8114.814941</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>9.971799</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.488146</td>\n",
       "      <td>0.238287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>1035705.0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>244.444443</td>\n",
       "      <td>2392.592529</td>\n",
       "      <td>9733.333008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>14.500839</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.079375</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year    comp_id       begin         end         amort    curr_assets  \\\n",
       "0  2013  1002029.0  2013-01-01  2013-12-31  14255.555664  217103.703125   \n",
       "1  2013  1011889.0  2013-01-01  2013-12-31  66125.929688  235114.812500   \n",
       "2  2013  1014183.0  2013-01-01  2013-12-31   6970.370605  209562.968750   \n",
       "3  2013  1022796.0  2013-01-01  2013-12-31    503.703705    3859.259277   \n",
       "4  2013  1035705.0  2013-01-01  2013-12-31    244.444443    2392.592529   \n",
       "\n",
       "       curr_liab  extra_exp  extra_inc  extra_profit_loss  ...  \\\n",
       "0  161174.078125        0.0        0.0                0.0  ...   \n",
       "1   16555.554688        0.0        0.0                0.0  ...   \n",
       "2    5703.703613        0.0        0.0                0.0  ...   \n",
       "3    8114.814941        0.0        0.0                0.0  ...   \n",
       "4    9733.333008        0.0        0.0                0.0  ...   \n",
       "\n",
       "   flag_high_ceo_age  flag_miss_ceo_age  ceo_young  labor_avg_mod  \\\n",
       "0                  0                  0          1       0.437500   \n",
       "1                  0                  0          0       1.583333   \n",
       "2                  0                  0          0       0.819444   \n",
       "3                  0                  0          0       0.083333   \n",
       "4                  0                  0          0       0.222222   \n",
       "\n",
       "   flag_miss_labor_avg  sales_mil_log_sq  flag_low_d1_sales_mil_log  \\\n",
       "0                    0          1.054824                          0   \n",
       "1                    0          0.666460                          0   \n",
       "2                    0          4.632597                          0   \n",
       "3                    0          9.971799                          0   \n",
       "4                    0         14.500839                          0   \n",
       "\n",
       "   flag_high_d1_sales_mil_log  d1_sales_mil_log_mod  d1_sales_mil_log_mod_sq  \n",
       "0                           0             -1.155013                 1.334055  \n",
       "1                           0              0.019109                 0.000365  \n",
       "2                           0             -0.110044                 0.012110  \n",
       "3                           0              0.488146                 0.238287  \n",
       "4                           0             -0.079375                 0.006300  \n",
       "\n",
       "[5 rows x 113 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09051331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ind2_cat\n",
       "26.0     735\n",
       "27.0     441\n",
       "28.0    1389\n",
       "29.0     179\n",
       "30.0     104\n",
       "33.0    1382\n",
       "55.0    1299\n",
       "56.0    8039\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firms_df[\"ind2_cat\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1810b18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "urban_m\n",
       "1.0    4278\n",
       "2.0    3872\n",
       "3.0    5418\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firms_df[\"urban_m\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10e17643",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2_catmat = patsy.dmatrix(\"C(ind2_cat, Treatment(reference=26))\", firms_df, return_type=\"dataframe\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ccf2929",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_region_locmat = patsy.dmatrix(\"C(m_region_loc, Treatment(reference='Central'))\", firms_df, return_type=\"dataframe\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f547a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_mmat = patsy.dmatrix(\"C(urban_m, Treatment(reference=1))\", firms_df, return_type=\"dataframe\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X1\n",
    "basevars = firms_df[[\"sales_mil_log\", \"sales_mil_log_sq\", \"d1_sales_mil_log_mod\", \"profit_loss_year_pl\"]]\n",
    "X1 = pd.concat([basevars, ind2_catmat], axis=1)\n",
    "\n",
    "# Define X2\n",
    "X2additional_vars = firms_df[[\"fixed_assets_bs\", \"share_eq_bs\",\"curr_liab_bs\", \"curr_liab_bs_flag_high\", \\\n",
    "                          \"curr_liab_bs_flag_error\",  \"age\", \"foreign_management\"]]\n",
    "X2 = pd.concat([X1, X2additional_vars], axis=1)\n",
    "\n",
    "# Define X3\n",
    "firm = pd.concat([firms_df[[\"age\", \"age2\", \"new\"]], ind2_catmat, m_region_locmat, urban_mmat], axis=1)\n",
    "X3 = pd.concat([firms_df[[\"sales_mil_log\", \"sales_mil_log_sq\"] + engvar + d1], firm], axis=1)\n",
    "\n",
    "# Define X4\n",
    "X4 = pd.concat([firms_df[[\"sales_mil_log\", \"sales_mil_log_sq\"] + engvar + d1 \\\n",
    "                                 + engvar2 + engvar3 + hr + qualityvars], firm], axis=1)\n",
    "\n",
    "# Define X5\n",
    "\n",
    "#Creat matrix for interactions1 variables\n",
    "int1mat = patsy.dmatrix(\"0 + C(ind2_cat):age + C(ind2_cat):age2 + C(ind2_cat):d1_sales_mil_log_mod \\\n",
    "                + C(ind2_cat):sales_mil_log + C(ind2_cat):ceo_age + C(ind2_cat):foreign_management \\\n",
    "                + C(ind2_cat):female + C(ind2_cat):C(urban_m) + C(ind2_cat):labor_avg_mod\", \n",
    "                        firms_df, return_type=\"dataframe\")\n",
    "\n",
    "#Drop first level to get k-1 dummies out of k categorical levels \n",
    "for col in int1mat.columns:\n",
    "    if col.startswith('C(ind2_cat)[26]') or col.endswith('C(urban_m)[1]'):\n",
    "        int1mat = int1mat.drop([col], axis=1)\n",
    "        \n",
    "#Creat matrix for interactions2 variables        \n",
    "int2mat = patsy.dmatrix(\"0 + sales_mil_log:age + sales_mil_log:female + sales_mil_log:profit_loss_year_pl \\\n",
    "                + sales_mil_log:foreign_management\", \n",
    "                        firms_df, return_type=\"dataframe\")\n",
    "\n",
    "X5 = pd.concat([X4, int1mat, int2mat], axis=1)\n",
    "\n",
    "# Define logitvars for LASSO\n",
    "logitvars = pd.concat([X4, int1mat, int2mat], axis=1)\n",
    "\n",
    "# Define rfvars for RF (no interactions, no modified features)\n",
    "rfvars  = pd.concat([firms_df[[\"sales_mil\", \"d1_sales_mil_log\"] + rawvars + hr + qualityvars], firm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5c268b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = firms_df[\"is_fast_growing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76268534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.2318691037735849)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ddc15eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Intercept',\n",
       " 'sales_mil_log',\n",
       " 'sales_mil_log_sq',\n",
       " 'd1_sales_mil_log_mod',\n",
       " 'profit_loss_year_pl',\n",
       " 'Intercept',\n",
       " 'C(ind2_cat, Treatment(reference=26))[T.27.0]',\n",
       " 'C(ind2_cat, Treatment(reference=26))[T.28.0]',\n",
       " 'C(ind2_cat, Treatment(reference=26))[T.29.0]',\n",
       " 'C(ind2_cat, Treatment(reference=26))[T.30.0]',\n",
       " 'C(ind2_cat, Treatment(reference=26))[T.33.0]',\n",
       " 'C(ind2_cat, Treatment(reference=26))[T.55.0]',\n",
       " 'C(ind2_cat, Treatment(reference=26))[T.56.0]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"Intercept\"] + list(X1.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62833502",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2267173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "def create_coef_matrix(X, model):\n",
    "    coef_matrix = pd.concat(\n",
    "        [pd.DataFrame(X.columns),pd.DataFrame(model.coef_.flatten())], axis = 1\n",
    "    )\n",
    "    coef_matrix.columns = ['variable', 'coefficient']\n",
    "    coef_matrix.iloc[-1] = ['Intercept', model.intercept_.flatten()[0]]\n",
    "    return coef_matrix\n",
    "\n",
    "def cv_summary(lambdas, C_values, model):\n",
    "    d = {'lambdas': lambdas, \n",
    "         'C_values': C_values, \n",
    "         'mean_cv_score': model.scores_[1].mean(axis = 0)}\n",
    "    return(pd.DataFrame(data=d))\n",
    "\n",
    "\"\"\"def create_roc_plot(y_true, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    all_coords = pd.DataFrame({\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thresholds\n",
    "    })\n",
    "    \n",
    "    plot = ggplot(all_coords, aes(x = 'fpr', y = 'tpr')) \\\n",
    "        + geom_line(color=color[0], size = 0.7) \\\n",
    "        + geom_area(position = 'identity', fill = 'mediumaquamarine', alpha = 0.3) \\\n",
    "        + xlab(\"False Positive Rate (1-Specifity)\") \\\n",
    "        + ylab(\"True Positive Rate (Sensitivity)\") \\\n",
    "        + geom_abline(intercept = 0, slope = 1,  linetype = \"dotted\", color = \"black\") \\\n",
    "        + scale_y_continuous(limits = (0, 1), breaks = seq(0, 1, .1), expand = (0, 0.01)) \\\n",
    "        + scale_x_continuous(limits = (0, 1), breaks = seq(0, 1, .1), expand = (0.01, 0)) \\\n",
    "        + theme_bw()\n",
    "    return(plot)\n",
    "\"\"\"\n",
    "\n",
    "def create_roc_plot(y_true, y_pred): # this is pretty important!\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred) # on x false positive and on y true positive\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Plot ROC curve line\n",
    "    ax.plot(fpr, tpr, color='k', linewidth=0.7)\n",
    "    \n",
    "    # Fill area under curve\n",
    "    ax.fill_between(fpr, tpr, alpha=0.3, color='white')\n",
    "    \n",
    "    # Add diagonal dotted line\n",
    "    ax.plot([0, 1], [0, 1], linestyle=':', color='black')\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('False Positive Rate (1-Specificity)')\n",
    "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "    \n",
    "    # Set axis limits and ticks\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    \n",
    "    # Style similar to theme_bw()\n",
    "    ax.grid(True, linestyle='-', alpha=0.2)\n",
    "    ax.set_facecolor('white')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def sigmoid_array(x):\n",
    "    return(1 / (1 + np.exp(-x)))\n",
    "\n",
    "def generate_fold_prediction(model, X, fold, param_index):\n",
    "    fold_coef = model.coefs_paths_[1][fold,param_index,:]\n",
    "    return(sigmoid_array(np.dot(X, np.transpose(fold_coef)[:-1]) +  np.transpose(fold_coef)[-1]))\n",
    "\n",
    "\"\"\"def create_loss_plot(all_coords, optimal_threshold, curr_exp_loss):\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['loss'] = (all_coords_copy.false_pos*FP + all_coords_copy.false_neg*FN)/all_coords_copy.n\n",
    "    \n",
    "    t = optimal_threshold\n",
    "    l = curr_exp_loss\n",
    "    \n",
    "    plot = ggplot(all_coords_copy, aes(x = 'thresholds', y = 'loss')) + \\\n",
    "        geom_line(color=color[0], size=0.7) + \\\n",
    "        scale_x_continuous(breaks = seq(0, 1.1, by = 0.1)) + \\\n",
    "        coord_cartesian(xlim=(0,1))+ \\\n",
    "        geom_vline(xintercept = t , color = color[0] ) + \\\n",
    "        annotate(geom = \"text\", x = t - 0.01, y= max(all_coords_copy.loss) - 0.4,\n",
    "                 label=\"best threshold: \" + str(round(t,2)),\n",
    "                 colour=color[1], angle=90, size = 7) +\\\n",
    "        annotate(geom = \"text\", x = t + 0.06, y= l,\\\n",
    "                 label= str(round(l, 2)), size = 7) +\\\n",
    "        theme_bw()\n",
    "    return(plot)\"\"\"\n",
    "\n",
    "def create_loss_plot(all_coords, optimal_threshold, curr_exp_loss): # what is optimal threshold here?\n",
    "    # Create copy and calculate loss\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['loss'] = (all_coords_copy.false_pos*FP + all_coords_copy.false_neg*FN)/all_coords_copy.n\n",
    "    \n",
    "    t = optimal_threshold\n",
    "    l = curr_exp_loss\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "    # Plot loss line\n",
    "    ax.plot(all_coords_copy['thresholds'], all_coords_copy['loss'], \n",
    "            color= 'k', linewidth=0.7)\n",
    "\n",
    "    # Add vertical line at optimal threshold\n",
    "    ax.axvline(x=t, color = 'k')\n",
    "\n",
    "    # Add annotations\n",
    "    ax.text(t - 0.04, max(all_coords_copy.loss) - 0.5,\n",
    "            f\"best threshold: {t:.2f}\", \n",
    "            color = 'k', \n",
    "            rotation=90, \n",
    "            fontsize = 9)\n",
    "    \n",
    "    ax.text(t + 0.06, l,\n",
    "            f\"{l:.2f}\",\n",
    "            fontsize = 9)\n",
    "\n",
    "    # Set x-axis ticks and limits\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "    # Style similar to theme_bw()\n",
    "    ax.grid(True, linestyle='-', alpha=0.2)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.set_xlabel('threshold')\n",
    "    ax.set_ylabel('loss')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"def create_roc_plot_with_optimal(all_coords, optimal_threshold):\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['sp'] = all_coords_copy.true_neg/all_coords_copy.neg\n",
    "    all_coords_copy['se'] = all_coords_copy.true_pos/all_coords_copy.pos\n",
    "    \n",
    "    best_coords = all_coords_copy[all_coords_copy.thresholds == optimal_threshold]\n",
    "    sp = best_coords.sp.values[0]\n",
    "    se = best_coords.se.values[0]\n",
    "\n",
    "    plot = ggplot(all_coords_copy, aes(x = 'sp', y = 'se')) +\\\n",
    "        geom_line(color=color[0], size=0.7) +\\\n",
    "        scale_y_continuous(breaks = seq(0, 1.1, by = 0.1)) +\\\n",
    "        scale_x_reverse(breaks = seq(0, 1.1, by = 0.1)) +\\\n",
    "        geom_point(data = pd.DataFrame({'sp': [sp], 'se': [se]})) +\\\n",
    "        annotate(geom = \"text\", x = sp, y = se + 0.03,\n",
    "                 label = str(round(sp, 2)) + ', ' + str(round(se, 2)), size = 7) +\\\n",
    "        theme_bw()\n",
    "    return(plot)\n",
    "\"\"\"\n",
    "def create_roc_plot_with_optimal(all_coords, optimal_threshold):\n",
    "    # Create copy and calculate metrics\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['sp'] = all_coords_copy.true_neg/all_coords_copy.neg\n",
    "    all_coords_copy['se'] = all_coords_copy.true_pos/all_coords_copy.pos\n",
    "    \n",
    "    # Get optimal point\n",
    "    best_coords = all_coords_copy[all_coords_copy.thresholds == optimal_threshold]\n",
    "    sp = best_coords.sp.values[0]\n",
    "    se = best_coords.se.values[0]\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax.plot(all_coords_copy['sp'], all_coords_copy['se'],\n",
    "            color='k', linewidth=0.9)\n",
    "    \n",
    "    # Add optimal point\n",
    "    ax.scatter([sp], [se], color='k', s = 100)\n",
    "    \n",
    "    # Add text annotation\n",
    "    ax.text(sp, se + 0.03,\n",
    "            f\"{sp:.2f}, {se:.2f}\",\n",
    "            fontsize = 9,\n",
    "            ha='center')\n",
    "    ax.text(sp - 0.02, se - 0.18,\n",
    "            'specificity (TNR) \\n& sensitivity (TPR) \\nat the best threshold',\n",
    "            fontsize = 9,\n",
    "            ha='center'\n",
    "           )\n",
    "    \n",
    "    # Set axis ticks and limits\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_xlabel('specificity')\n",
    "    ax.set_ylabel('sensitivity')\n",
    "    \n",
    "    # Reverse x-axis\n",
    "    ax.set_xlim(1, 0)\n",
    "    \n",
    "    # Style similar to theme_bw()\n",
    "    ax.grid(True, linestyle='-', alpha=0.2)\n",
    "    ax.set_facecolor('white')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0e19b",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4432d325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td colspan=\"1\"><em>Dependent variable: is_fast_growing</em></td></tr><tr><td style=\"text-align:left\"></td><tr><td style=\"text-align:left\"></td><td>(1)</td></tr>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "\n",
       "<tr><td style=\"text-align:left\">Intercept</td><td>0.083<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.007)</td></tr>\n",
       "<tr><td style=\"text-align:left\">sales_mil_log</td><td>-0.007<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.004)</td></tr>\n",
       "<tr><td style=\"text-align:left\">C(ind2_cat, Treatment(reference=26))[T.55.0]</td><td>-0.026<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.016)</td></tr>\n",
       "<tr><td style=\"text-align:left\">C(ind2_cat, Treatment(reference=26))[T.56.0]</td><td>0.002<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.013)</td></tr>\n",
       "<tr><td style=\"text-align:left\">sales_mil_log_sq</td><td>0.006<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.001)</td></tr>\n",
       "<tr><td style=\"text-align:left\">d1_sales_mil_log_mod</td><td>0.549<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.007)</td></tr>\n",
       "<tr><td style=\"text-align:left\">profit_loss_year_pl</td><td>-0.067<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.011)</td></tr>\n",
       "<tr><td style=\"text-align:left\">Intercept</td><td>0.083<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.007)</td></tr>\n",
       "<tr><td style=\"text-align:left\">C(ind2_cat, Treatment(reference=26))[T.27.0]</td><td>0.021<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.021)</td></tr>\n",
       "<tr><td style=\"text-align:left\">C(ind2_cat, Treatment(reference=26))[T.28.0]</td><td>0.022<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.016)</td></tr>\n",
       "<tr><td style=\"text-align:left\">C(ind2_cat, Treatment(reference=26))[T.29.0]</td><td>0.026<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.029)</td></tr>\n",
       "<tr><td style=\"text-align:left\">C(ind2_cat, Treatment(reference=26))[T.30.0]</td><td>0.067<sup>*</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.036)</td></tr>\n",
       "<tr><td style=\"text-align:left\">C(ind2_cat, Treatment(reference=26))[T.33.0]</td><td>0.041<sup>**</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.016)</td></tr>\n",
       "\n",
       "<td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align: left\">Observations</td><td>13568</td></tr><tr><td style=\"text-align: left\">R<sup>2</sup></td><td>0.326</td></tr><tr><td style=\"text-align: left\">Adjusted R<sup>2</sup></td><td>0.326</td></tr><tr><td style=\"text-align: left\">Residual Std. Error</td><td>0.347 (df=13556)</td></tr><tr><td style=\"text-align: left\">F Statistic</td><td>596.210<sup>***</sup> (df=11; 13556)</td></tr>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td><td colspan=\"1\" style=\"text-align: right\"><sup>*</sup>p&lt;0.1; <sup>**</sup>p&lt;0.05; <sup>***</sup>p&lt;0.01</td></tr></table>"
      ],
      "text/plain": [
       "<stargazer.stargazer.Stargazer at 0x7557d97a63c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols_modelx1 = smf.ols(\"y ~ X1\", data=firms_df).fit()\n",
    "ols1_summary = Stargazer([ols_modelx1])\n",
    "ols1_summary.dependent_variable_name(\"is_fast_growing\")\n",
    "ols_modelx1_param_names = ols_modelx1.params.index.tolist()\n",
    "ols1_summary.rename_covariates(dict(zip(ols_modelx1_param_names, [\"Intercept\"] + list(X1.columns))))\n",
    "ols1_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26074ffe",
   "metadata": {},
   "source": [
    "### OLS with train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b24865a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "smp_size = round(0.2 * firms_df.shape[0])-1\n",
    "\n",
    "# train - test split\n",
    "df_train, df_test=train_test_split(firms_df, test_size=smp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51c30163",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_modelx1_test, r2_modelx1_test, pred_modelx1_test = [], [], []\n",
    "rmse_modelx1_train, r2_modelx1_train, pred_modelx1_train = [], [], []\n",
    "\n",
    "\n",
    "k = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in k.split(firms_df):\n",
    "    # Select and add constant to X\n",
    "    X_train = sm.add_constant(X1.iloc[train_index])\n",
    "    X_test = sm.add_constant(X1.iloc[test_index])\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit and Predict\n",
    "    mod1 = sm.OLS(y_train, X_train).fit()\n",
    "    y_pred_test = mod1.predict(X_test) # Use X_test here\n",
    "    y_pred_train = mod1.predict(X_train)\n",
    "    \n",
    "    # Store results\n",
    "    pred_modelx1_test.append(np.mean(y_pred_test))\n",
    "    rmse_modelx1_test.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    r2_modelx1_test.append(r2_score(y_test, y_pred_test))\n",
    "    \n",
    "    pred_modelx1_train.append(np.mean(y_pred_train))\n",
    "    rmse_modelx1_train.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    r2_modelx1_train.append(r2_score(y_train, y_pred_train))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e1a46ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict train</th>\n",
       "      <th>r2 train</th>\n",
       "      <th>rmse train</th>\n",
       "      <th>predict test</th>\n",
       "      <th>r2 test</th>\n",
       "      <th>rmse test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.232080</td>\n",
       "      <td>0.324471</td>\n",
       "      <td>0.346976</td>\n",
       "      <td>0.226664</td>\n",
       "      <td>0.331744</td>\n",
       "      <td>0.344554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.232541</td>\n",
       "      <td>0.323224</td>\n",
       "      <td>0.347536</td>\n",
       "      <td>0.235280</td>\n",
       "      <td>0.336454</td>\n",
       "      <td>0.342375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.228764</td>\n",
       "      <td>0.326587</td>\n",
       "      <td>0.344689</td>\n",
       "      <td>0.234068</td>\n",
       "      <td>0.323046</td>\n",
       "      <td>0.353516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.326725</td>\n",
       "      <td>0.345711</td>\n",
       "      <td>0.234714</td>\n",
       "      <td>0.322874</td>\n",
       "      <td>0.349550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.235191</td>\n",
       "      <td>0.329732</td>\n",
       "      <td>0.347225</td>\n",
       "      <td>0.228416</td>\n",
       "      <td>0.309042</td>\n",
       "      <td>0.343535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.231869</td>\n",
       "      <td>0.326148</td>\n",
       "      <td>0.346428</td>\n",
       "      <td>0.231828</td>\n",
       "      <td>0.324632</td>\n",
       "      <td>0.346706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         predict train  r2 train  rmse train  predict test   r2 test  \\\n",
       "0             0.232080  0.324471    0.346976      0.226664  0.331744   \n",
       "1             0.232541  0.323224    0.347536      0.235280  0.336454   \n",
       "2             0.228764  0.326587    0.344689      0.234068  0.323046   \n",
       "3             0.230769  0.326725    0.345711      0.234714  0.322874   \n",
       "4             0.235191  0.329732    0.347225      0.228416  0.309042   \n",
       "Average       0.231869  0.326148    0.346428      0.231828  0.324632   \n",
       "\n",
       "         rmse test  \n",
       "0         0.344554  \n",
       "1         0.342375  \n",
       "2         0.353516  \n",
       "3         0.349550  \n",
       "4         0.343535  \n",
       "Average   0.346706  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_modelx1 = {\n",
    "        \"predict train\": pred_modelx1_train,\n",
    "        \"r2 train\": r2_modelx1_train,\n",
    "        \"rmse train\": rmse_modelx1_train,\n",
    "        \"predict test\": pred_modelx1_test,\n",
    "        \"r2 test\": r2_modelx1_test,\n",
    "        \"rmse test\": rmse_modelx1_test\n",
    "    }\n",
    "\n",
    "results_modelx1 = pd.concat([pd.DataFrame(results_modelx1), pd.DataFrame(pd.DataFrame(results_modelx1).mean(), columns=[\"Average\"]).T])\n",
    "results_modelx1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "42a3f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3f983",
   "metadata": {},
   "source": [
    "### Logistic Regression with Cross-Validation for X1:X5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c033b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train, index_holdout= train_test_split(\n",
    "    firms_df.index.values, train_size=round(0.8*len(firms_df.index)), random_state=42)\n",
    "\n",
    "y_train = y[index_train]\n",
    "y_holdout = y[index_holdout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d0a102a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model_vars = [X1.loc[index_train], X2.loc[index_train], X3.loc[index_train], X4.loc[index_train], X5.loc[index_train]]\n",
    "\n",
    "logit_models = dict()\n",
    "CV_RMSE_folds = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "728ec222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-07 15:41:25.282933 Running regression 0...\n",
      "2026-02-07 15:41:25.451869 Running regression 1...\n",
      "2026-02-07 15:41:25.880724 Running regression 2...\n",
      "2026-02-07 15:41:31.308692 Running regression 3...\n",
      "2026-02-07 15:42:05.407982 Running regression 4...\n"
     ]
    }
   ],
   "source": [
    "#### runs for more than 3 minutes!!! ####\n",
    "import datetime\n",
    "logit_r2 = {}\n",
    "\n",
    "\n",
    "for i in range(len(logit_model_vars)):\n",
    "    print(datetime.datetime.now(), f'Running regression {i}...')\n",
    "    LRCV_brier = LogisticRegressionCV(\n",
    "        Cs = [1e20], \n",
    "        cv = k, # simply the number of folds\n",
    "        refit = True, \n",
    "        scoring = 'neg_brier_score', \n",
    "        solver = \"newton-cg\", \n",
    "        tol=1e-7, \n",
    "        random_state = 20250224)\n",
    "    logit_models['X'+str(i+1)] = LRCV_brier.fit(logit_model_vars[i], y_train)\n",
    "    \n",
    "    # Calculate RMSE on test for each fold\n",
    "    CV_RMSE_folds['X'+str(i+1)] = np.sqrt(-1*(logit_models['X'+str(i+1)].scores_[1])).flatten()\n",
    "    logit_r2['X'+str(i+1)] = logit_models['X'+str(i+1)].score(logit_model_vars[i], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5dd92899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.421394</td>\n",
       "      <td>0.421508</td>\n",
       "      <td>0.422607</td>\n",
       "      <td>0.422571</td>\n",
       "      <td>0.424482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.423291</td>\n",
       "      <td>0.423447</td>\n",
       "      <td>0.423769</td>\n",
       "      <td>0.425082</td>\n",
       "      <td>0.425385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.419140</td>\n",
       "      <td>0.419183</td>\n",
       "      <td>0.419941</td>\n",
       "      <td>0.419957</td>\n",
       "      <td>0.421114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.426344</td>\n",
       "      <td>0.426338</td>\n",
       "      <td>0.427292</td>\n",
       "      <td>0.428739</td>\n",
       "      <td>0.430256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.419368</td>\n",
       "      <td>0.419979</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>0.420025</td>\n",
       "      <td>0.422060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.421907</td>\n",
       "      <td>0.422091</td>\n",
       "      <td>0.422721</td>\n",
       "      <td>0.423275</td>\n",
       "      <td>0.424659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R2</th>\n",
       "      <td>-0.177448</td>\n",
       "      <td>-0.177386</td>\n",
       "      <td>-0.177174</td>\n",
       "      <td>-0.176449</td>\n",
       "      <td>-0.175276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1        X2        X3        X4        X5\n",
       "0        0.421394  0.421508  0.422607  0.422571  0.424482\n",
       "1        0.423291  0.423447  0.423769  0.425082  0.425385\n",
       "2        0.419140  0.419183  0.419941  0.419957  0.421114\n",
       "3        0.426344  0.426338  0.427292  0.428739  0.430256\n",
       "4        0.419368  0.419979  0.419998  0.420025  0.422060\n",
       "Average  0.421907  0.422091  0.422721  0.423275  0.424659\n",
       "R2      -0.177448 -0.177386 -0.177174 -0.176449 -0.175276"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rmse_folds = pd.DataFrame(CV_RMSE_folds)\n",
    "glm_model_overview = pd.concat([cv_rmse_folds, pd.DataFrame(cv_rmse_folds.mean(), columns = [\"Average\"]).T,\n",
    "                                pd.DataFrame(logit_r2, index= [\"R2\"])])\n",
    "glm_model_overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603705c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### honestly, all of the models are kind of equally bad; any difference between them might just be random\n",
    "### still, going off the numbers, we'll want to pick X1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa5187",
   "metadata": {},
   "source": [
    "### Lasso Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3152cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we normalize the lasso variables\n",
    "normalized_logitvars = pd.DataFrame(StandardScaler().fit_transform(logitvars.loc[index_train]))\n",
    "normalized_logitvars.columns = logitvars.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7d1227d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas=list(10**np.arange(-1,-4.01, -1/3))\n",
    "n_obs = normalized_logitvars.shape[0]*4/5\n",
    "Cs_values = [1/(l*n_obs) for l in lambdas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "556b369a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.00115164916159941),\n",
       " np.float64(0.002481152904495904),\n",
       " np.float64(0.0053454818887193395),\n",
       " np.float64(0.011516491615994096),\n",
       " np.float64(0.024811529044959025),\n",
       " np.float64(0.053454818887193334),\n",
       " np.float64(0.1151649161599409),\n",
       " np.float64(0.24811529044959024),\n",
       " np.float64(0.5345481888719334),\n",
       " np.float64(1.1516491615994078)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cs_values # the strength of the regularization -> supressing unimportant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "99bca4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logLasso = LogisticRegressionCV(\n",
    "    Cs = Cs_values, \n",
    "    penalty = 'l1', # L1 makes it lasso\n",
    "    cv = k, \n",
    "    refit = True, \n",
    "    scoring = 'accuracy', \n",
    "    solver = 'liblinear',\n",
    "    random_state = 20250224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8de44e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason y_train has one more row than normalized_logitvars; I just cut it\n",
    "logit_models[\"LASSO\"] = logLasso.fit(normalized_logitvars, y_train[:len(y_train)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2448375f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambdas</th>\n",
       "      <th>C_values</th>\n",
       "      <th>mean_cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.767183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.046416</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.767183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021544</td>\n",
       "      <td>0.005345</td>\n",
       "      <td>0.767183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.011516</td>\n",
       "      <td>0.767183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.024812</td>\n",
       "      <td>0.767091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.053455</td>\n",
       "      <td>0.766999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.115165</td>\n",
       "      <td>0.766999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.248115</td>\n",
       "      <td>0.766999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.534548</td>\n",
       "      <td>0.766723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.151649</td>\n",
       "      <td>0.766354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lambdas  C_values  mean_cv_score\n",
       "0  0.100000  0.001152       0.767183\n",
       "1  0.046416  0.002481       0.767183\n",
       "2  0.021544  0.005345       0.767183\n",
       "3  0.010000  0.011516       0.767183\n",
       "4  0.004642  0.024812       0.767091\n",
       "5  0.002154  0.053455       0.766999\n",
       "6  0.001000  0.115165       0.766999\n",
       "7  0.000464  0.248115       0.766999\n",
       "8  0.000215  0.534548       0.766723\n",
       "9  0.000100  1.151649       0.766354"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_summary_lasso = cv_summary(lambdas, Cs_values, logit_models[\"LASSO\"])\n",
    "cv_summary_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "96668365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refit with negative brier score so we have RMSE values for the same cv split\n",
    "#### takes \n",
    "\n",
    "logLasso_brier = LogisticRegressionCV(\n",
    "    Cs = Cs_values, \n",
    "    penalty = 'l1', \n",
    "    cv = k, \n",
    "    refit = True, \n",
    "    scoring = 'neg_brier_score', # now negative; before we optimized based on accuracy\n",
    "    solver = \"liblinear\", \n",
    "    random_state = 20250224)\n",
    "logLasso_brier_fitted = logLasso_brier.fit(normalized_logitvars, y_train[:len(y_train)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "35360e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.1)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lambda = cv_summary_lasso.sort_values('mean_cv_score', ascending = False).iloc[0,0]\n",
    "best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d4e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_loglasso = {}\n",
    "\n",
    "for i, l in enumerate(lambdas):\n",
    "    if l == best_lambda:\n",
    "        best_lambda_i = i\n",
    "        CV_RMSE_folds['LASSO'] = np.sqrt(-1*(logLasso_brier_fitted.scores_[1][:,i])).tolist()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536f72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>LASSO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.421394</td>\n",
       "      <td>0.421508</td>\n",
       "      <td>0.422607</td>\n",
       "      <td>0.422571</td>\n",
       "      <td>0.424482</td>\n",
       "      <td>0.436054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.423291</td>\n",
       "      <td>0.423447</td>\n",
       "      <td>0.423769</td>\n",
       "      <td>0.425082</td>\n",
       "      <td>0.425385</td>\n",
       "      <td>0.434986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.419140</td>\n",
       "      <td>0.419183</td>\n",
       "      <td>0.419941</td>\n",
       "      <td>0.419957</td>\n",
       "      <td>0.421114</td>\n",
       "      <td>0.434683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.426344</td>\n",
       "      <td>0.426338</td>\n",
       "      <td>0.427292</td>\n",
       "      <td>0.428739</td>\n",
       "      <td>0.430256</td>\n",
       "      <td>0.433779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.419368</td>\n",
       "      <td>0.419979</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>0.420025</td>\n",
       "      <td>0.422060</td>\n",
       "      <td>0.432025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average</th>\n",
       "      <td>0.421907</td>\n",
       "      <td>0.422091</td>\n",
       "      <td>0.422721</td>\n",
       "      <td>0.423275</td>\n",
       "      <td>0.424659</td>\n",
       "      <td>0.434305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1        X2        X3        X4        X5     LASSO\n",
       "0        0.421394  0.421508  0.422607  0.422571  0.424482  0.436054\n",
       "1        0.423291  0.423447  0.423769  0.425082  0.425385  0.434986\n",
       "2        0.419140  0.419183  0.419941  0.419957  0.421114  0.434683\n",
       "3        0.426344  0.426338  0.427292  0.428739  0.430256  0.433779\n",
       "4        0.419368  0.419979  0.419998  0.420025  0.422060  0.432025\n",
       "Average  0.421907  0.422091  0.422721  0.423275  0.424659  0.434305"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglasso_overview = pd.DataFrame(CV_RMSE_folds)\n",
    "\n",
    "loglasso_overview = pd.concat([loglasso_overview, pd.DataFrame(loglasso_overview.mean(), columns = [\"Average\"]).T])\n",
    "loglasso_overview\n",
    "\n",
    "# TODO: one could add an R2 to this\n",
    "# but really, LASSO is gonna be worse regardless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48940ef",
   "metadata": {},
   "source": [
    "# PART II: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7ef7c",
   "metadata": {},
   "source": [
    "Think about the business problem, and define your loss function (like FP=X dollars, FN=Y dollars).\n",
    "\n",
    "Idea 1: We have some spare money and want to do some investments. Overall, riskier firms, thus firms with a higher probability to default also pay higher returns. On the other hand, we lose money when a risky firm defaults. The money lost from an unexpected default is about the same, as money lost from a risky firm that ends up well performing that we decided not to invest in.\n",
    "Therefore, a suggested loss function would be:\n",
    "FP = 0.5 FN = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea9dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3efcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bb482ad",
   "metadata": {},
   "source": [
    "## MODELS WE CAN'T USE  :/\n",
    "\n",
    "My dumb ass forgot that we have binary outcome variables and calculated all the regular models.\n",
    "\n",
    "For now, I'm keeping them in the script in case that some  of the syntax might come in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592f622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abd06e45",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# define model\n",
    "model = Lasso()\n",
    "\n",
    "grid = dict()\n",
    "grid[\"alpha\"] = np.arange(0.05, 1, 0.05)\n",
    "# define search\n",
    "search = GridSearchCV(model, grid, scoring=\"neg_root_mean_squared_error\", cv = k, verbose= 3) # control your output with the 'verbose' option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for both sets\n",
    "rmse_lasso_test, r2_lasso_test = [], []\n",
    "rmse_lasso_train, r2_lasso_train = [], []\n",
    "pred_lasso_test, pred_lasso_train = [], []\n",
    "\n",
    "k = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in k.split(rfvars):\n",
    "    \n",
    "    X_train, X_test = logitvars.iloc[train_index], logitvars.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    ### LASSO MODEL ###\n",
    "    lasso_mod = search.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_test = lasso_mod.predict(X_test)\n",
    "    y_pred_train = lasso_mod.predict(X_train)\n",
    "    \n",
    "    pred_lasso_test.append(y_pred_test.mean())\n",
    "    pred_lasso_train.append(y_pred_train.mean())\n",
    "\n",
    "    rmse_lasso_test.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    r2_lasso_test.append(r2_score(y_test, y_pred_test))\n",
    "    \n",
    "    rmse_lasso_train.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    r2_lasso_train.append(r2_score(y_train, y_pred_train))\n",
    "\n",
    "# Quick summary of the averages\n",
    "print(f\"Train RMSE: {np.mean(rmse_lasso_train):.4f} vs Test RMSE: {np.mean(rmse_lasso_test):.4f}\")\n",
    "print(f\"Train R2:   {np.mean(r2_lasso_train):.4f} vs Test R2:   {np.mean(r2_lasso_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ddbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lasso_mod = {\n",
    "        \"predicted train\": pred_lasso_train,\n",
    "        \"r2 train\": r2_lasso_train,\n",
    "        \"rmse train\": rmse_lasso_train,\n",
    "        \"predicted test\": pred_lasso_test,\n",
    "        \"r2 test\": r2_lasso_test,\n",
    "        \"rmse test\": pred_lasso_test\n",
    "    }\n",
    "results_lasso_mod = pd.concat([pd.DataFrame(results_lasso_mod), pd.DataFrame(pd.DataFrame(results_lasso_mod).mean(), columns=[\"Average\"]).T])\n",
    "results_lasso_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10def78a",
   "metadata": {},
   "source": [
    "### RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7124297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(random_state = 20250224)\n",
    "tune_grid = {\"max_features\": [6, 8, 10, 12], \"min_samples_leaf\": [5, 10, 15]}\n",
    "\n",
    "rf_random = GridSearchCV(\n",
    "    estimator = rfr,\n",
    "    param_grid = tune_grid,\n",
    "    cv = 5,\n",
    "    scoring = \"neg_root_mean_squared_error\",\n",
    "    verbose = 3,\n",
    ")\n",
    "# Built into grid search, it will run on the test set, not on the train set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e39130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch out, this takes 10 minutes to run!\n",
    " \n",
    "rmse_rf_test, r2_rf_test = [], []\n",
    "rmse_rf_train, r2_rf_train = [], []\n",
    "pred_rf_test, pred_rf_train = [], []\n",
    "\n",
    "k = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in k.split(rfvars):\n",
    "    \n",
    "    X_train, X_test = rfvars.iloc[train_index], rfvars.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    ### Random Forest Model ###\n",
    "    rf_mod = rf_random.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_test = rf_mod.predict(X_test)\n",
    "    y_pred_train = rf_mod.predict(X_train)\n",
    "    \n",
    "    pred_rf_test.append(y_pred_test.mean())\n",
    "    pred_rf_train.append(y_pred_train.mean())\n",
    "\n",
    "    rmse_rf_test.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    r2_rf_test.append(r2_score(y_test, y_pred_test))\n",
    "    \n",
    "    rmse_rf_train.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    r2_rf_train.append(r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ee8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf_mod = {\n",
    "        \"predicted train\": pred_rf_train,\n",
    "        \"r2 train\": r2_rf_train,\n",
    "        \"rmse train\": rmse_rf_train,\n",
    "        \"predicted test\": pred_rf_test,\n",
    "        \"r2 test\": r2_rf_test,\n",
    "        \"rmse test\": pred_rf_test\n",
    "    }\n",
    "results_rf_mod = pd.concat([pd.DataFrame(results_rf_mod), pd.DataFrame(pd.DataFrame(results_rf_mod).mean(), columns=[\"Average\"]).T])\n",
    "results_rf_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6913ad",
   "metadata": {},
   "source": [
    "### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cart = DecisionTreeRegressor(random_state=1234, criterion=\"squared_error\",max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cart_test, r2_cart_test = [], []\n",
    "rmse_cart_train, r2_cart_train = [], []\n",
    "pred_cart_test, pred_cart_train = [], []\n",
    "\n",
    "k = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in k.split(rfvars):\n",
    "    \n",
    "    X_train, X_test = rfvars.iloc[train_index], rfvars.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    ### Random Forest Model ###\n",
    "    cart_mod = cart.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_test = rf_mod.predict(X_test)\n",
    "    y_pred_train = rf_mod.predict(X_train)\n",
    "    \n",
    "    pred_cart_test.append(y_pred_test.mean())\n",
    "    pred_cart_train.append(y_pred_train.mean())\n",
    "\n",
    "    rmse_cart_test.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    r2_cart_test.append(r2_score(y_test, y_pred_test))\n",
    "    \n",
    "    rmse_cart_train.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    r2_cart_train.append(r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d378213",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cart_mod = {\n",
    "        \"predicted train\": pred_cart_train,\n",
    "        \"r2 train\": r2_cart_train,\n",
    "        \"rmse train\": rmse_cart_train,\n",
    "        \"predicted test\": pred_cart_test,\n",
    "        \"r2 test\": r2_cart_test,\n",
    "        \"rmse test\": pred_cart_test\n",
    "    }\n",
    "results_cart_mod = pd.concat([pd.DataFrame(results_cart_mod), pd.DataFrame(pd.DataFrame(results_cart_mod).mean(), columns=[\"Average\"]).T])\n",
    "results_cart_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a1942",
   "metadata": {},
   "source": [
    "### BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8f099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor(learning_rate=0.1, min_samples_split=20, max_features = 10\n",
    "                                #, n_estimators = 50\n",
    "                               )\n",
    "\n",
    "tune_grid = {\"n_estimators\": [200, 300], \"max_depth\": [5, 10]}\n",
    "\n",
    "gbm_model_cv = GridSearchCV(\n",
    "    gbm,\n",
    "    tune_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=10,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c8e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Flatten categorical_columns and ensure no nested lists\n",
    "# We use a list comprehension to make sure we only grab strings\n",
    "raw_cat_list = engvar3 + [\"balsheet_notfullyear\", \"foreign_management\"]\n",
    "categorical_columns = []\n",
    "for item in raw_cat_list:\n",
    "    if isinstance(item, list):\n",
    "        categorical_columns.extend(item)\n",
    "    else:\n",
    "        categorical_columns.append(item)\n",
    "\n",
    "# 2. Flatten all_vars the same way\n",
    "final_all_vars = []\n",
    "for item in all_vars:\n",
    "    if isinstance(item, list):\n",
    "        final_all_vars.extend(item)\n",
    "    else:\n",
    "        final_all_vars.append(item)\n",
    "\n",
    "# 3. Filter numerical columns based on the flattened lists\n",
    "numerical_columns = [col for col in final_all_vars if col not in categorical_columns]\n",
    "\n",
    "# 4. Redefine Preprocessing\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_columns),\n",
    "        (\"num\", \"passthrough\", numerical_columns),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now try the fit again\n",
    "gbm_pipe = Pipeline([(\"preprocess\", preprocessing), (\"regressor\", gbm_model_cv)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6721a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch out this takes 10 min to run!\n",
    "#\n",
    "r2_gbm_test, r2_gbm_train = [], []\n",
    "rmse_gbm_test, rmse_gbm_train = [], []\n",
    "pred_gbm_test, pred_gbm_train = [], []\n",
    "\n",
    "for train_index, test_index in k.split(firms_df[final_all_vars]):\n",
    "    \n",
    "    X_train, X_test = firms_df[final_all_vars].iloc[train_index], firms_df[final_all_vars].iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # 1. Fit the model\n",
    "    gbm_mod = gbm_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # 2. Predict for TRAIN and calculate metrics\n",
    "    y_pred_train = gbm_mod.predict(X_train)  # <--- Define this!\n",
    "    pred_gbm_train.append(y_pred_train.mean())\n",
    "    rmse_gbm_train.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    r2_gbm_train.append(r2_score(y_train, y_pred_train))\n",
    "\n",
    "    # 3. Predict for TEST and calculate metrics\n",
    "    y_pred_test = gbm_mod.predict(X_test)    # <--- Define this!\n",
    "    pred_gbm_test.append(y_pred_test.mean())\n",
    "    rmse_gbm_test.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    r2_gbm_test.append(r2_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb54043",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gbm_mod = {\n",
    "        \"predicted train\": pred_gbm_train,\n",
    "        \"r2 train\": r2_gbm_train,\n",
    "        \"rmse train\": rmse_gbm_train,\n",
    "        \"predicted test\": pred_gbm_test,\n",
    "        \"r2 test\": r2_gbm_test,\n",
    "        \"rmse test\": pred_gbm_test\n",
    "    }\n",
    "results_gbm_mod = pd.concat([pd.DataFrame(results_gbm_mod), pd.DataFrame(pd.DataFrame(results_gbm_mod).mean(), columns=[\"Average\"]).T])\n",
    "#pd.DataFrame(results_gbm_mod)\n",
    "results_gbm_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcb15b4",
   "metadata": {},
   "source": [
    "### GLM model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch out, this takes 10 minutes to run!\n",
    " \n",
    "rmse_glm_test, r2_glm_test = [], []\n",
    "rmse_glm_train, r2_glm_train = [], []\n",
    "pred_glm_test, pred_glm_train = [], []\n",
    "\n",
    "k = KFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "for train_index, test_index in k.split(rfvars):\n",
    "    \n",
    "    X_train, X_test = rfvars.iloc[train_index], rfvars.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    ### GLM ###\n",
    "    glm_modelx1 = LogisticRegression(\n",
    "    solver = \"newton-cg\", \n",
    "    max_iter = 1000, \n",
    "    penalty = None, \n",
    "    random_state = 1234).fit(X_train, y_train)\n",
    "    #regression_results(y, glm_modelx1.predict(X1))\n",
    "\n",
    "    y_pred_test = glm_modelx1.predict(X_test)\n",
    "    y_pred_train = glm_modelx1.predict(X_train)\n",
    "    \n",
    "    pred_glm_test.append(y_pred_test.mean())\n",
    "    pred_glm_train.append(y_pred_train.mean())\n",
    "\n",
    "    rmse_glm_test.append(np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    r2_glm_test.append(r2_score(y_test, y_pred_test))\n",
    "    \n",
    "    rmse_glm_train.append(np.sqrt(mean_squared_error(y_train, y_pred_train)))\n",
    "    r2_glm_train.append(r2_score(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_glm_modelx1 = {\n",
    "        \"predicted train\": pred_glm_train,\n",
    "        \"r2 train\": r2_glm_train,\n",
    "        \"rmse train\": rmse_glm_train,\n",
    "        \"predicted test\": pred_glm_test,\n",
    "        \"r2 test\": r2_glm_test,\n",
    "        \"rmse test\": pred_glm_test\n",
    "    }\n",
    "results_glm_modelx1 = pd.concat([pd.DataFrame(results_glm_modelx1), pd.DataFrame(pd.DataFrame(results_glm_modelx1).mean(), columns=[\"Average\"]).T])\n",
    "results_glm_modelx1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4922fb",
   "metadata": {},
   "source": [
    "### comparing all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "## comparing all models:\n",
    "\n",
    "model_comparison = pd.DataFrame({'model': ['OLS', 'LASSO', \"CART\", 'GBM', 'RF', \"GLM1\"],\n",
    "    'RMSE': [np.mean(rmse_modelx1_train), np.mean(rmse_lasso_train),\n",
    "            np.mean(rmse_cart_train), np.mean(rmse_gbm_train), np.mean(rmse_rf_train),\n",
    "            np.mean(rmse_glm_train)],\n",
    "    \"R2\": [np.mean(r2_modelx1_train), np.mean(r2_lasso_train),\n",
    "            np.mean(r2_cart_train), np.mean(r2_gbm_train), np.mean(r2_rf_train),\n",
    "            np.mean(r2_glm_train)]\n",
    "})\n",
    "\n",
    "print(\"The Random Forest model works best in both RMSE and R2\")\n",
    "model_comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
